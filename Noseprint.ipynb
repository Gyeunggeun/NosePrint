{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T08:56:36.343479500Z",
     "start_time": "2023-05-11T08:56:27.086646500Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 강아지를 비문으로 인식할 수 있는 다중 분류 딥러닝 모델\n",
    "# 패키지 불러오기\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rd\n",
    "import cv2, os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import *\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.backend import clear_session\n",
    "from keras.optimizers import Adam\n",
    "from keras.datasets import mnist, fashion_mnist\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # json 파일 불러오기\n",
    "# import os\n",
    "#\n",
    "# json_files = []\n",
    "#\n",
    "# for i in range(1, 25):\n",
    "#     if i not in [21, 23]:\n",
    "#         json_file = f\"D:\\\\Dog\\\\noseprints\\\\data\\\\a{i:03d}.json\"\n",
    "#         json_files.append(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "# import os\n",
    "#\n",
    "# # JSON 파일 범위 지정\n",
    "# json_range = list(range(1, 25))\n",
    "# json_range.remove(21)  # a021.json 제외\n",
    "# json_range.remove(23)  # a023.json 제외\n",
    "#\n",
    "# base_path = \"D:/Dog/noseprints/data\" # base_path를 현재 작업물이 있는 폴더로 지정\n",
    "#\n",
    "# for i in json_range:\n",
    "#     input_file = os.path.join(base_path, f\"a{i:03d}.json\")\n",
    "#\n",
    "#     # JSON 파일 읽기\n",
    "#     with open(input_file, 'r', encoding='utf-8') as file:\n",
    "#         data = json.load(file)\n",
    "#\n",
    "#     # \"file_name\" 필드 수정\n",
    "#     for image in data[\"images\"]:\n",
    "#         image_id = image[\"id\"]\n",
    "#         new_file_name = f\"a{i:03d}_{image_id}.jpg\"\n",
    "#         image[\"file_name\"] = new_file_name\n",
    "#\n",
    "#     # 변경된 내용을 파일에 다시 저장\n",
    "#     output_file = input_file\n",
    "#     with open(output_file, 'w', encoding='utf-8') as file:\n",
    "#         json.dump(data, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "# merged_data = []\n",
    "#\n",
    "# # 각 json 파일을 읽고 병합\n",
    "# for json_file in json_files:\n",
    "#     with open(json_file, 'r', encoding='utf-8') as file:\n",
    "#         data = json.load(file)\n",
    "#         merged_data.append(data)\n",
    "#\n",
    "# output_path = \"D:\\\\Dog\\\\noseprints\\\\data\"\n",
    "# os.makedirs(output_path, exist_ok=True)\n",
    "#\n",
    "# # 병합된 데이터를 D:\\Dog\\noseprints\\data\\labels\\label.json 파일에 저장\n",
    "# output_file = os.path.join(output_path, \"label.json\")\n",
    "# with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "#     json.dump(merged_data, outfile, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# python example.py --datasets COCO --img_path ../noseprints/data/images/a024  --label ../noseprints/data/a024.json --convert_output_path ../noseprints/data/labels --img_type '.jpg' --manifest_path ./ --cls_list_file ../noseprints/class.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # json 파일을 YOLO 포맷으로 변환\n",
    "# import subprocess\n",
    "#\n",
    "# def run_command(img_path, label_path):\n",
    "#     command = f\"python D:/Dog/convert2Yolo/example.py --datasets COCO --img_path {img_path} --label {label_path} --convert_output_path D:/Dog/noseprints/data/labels --img_type '.jpg' --manifest_path D:/Dog --cls_list_file D:/Dog/noseprints/class.names\"\n",
    "#\n",
    "#     result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True, text=True)\n",
    "#\n",
    "#     print(result.stdout)\n",
    "#     print(result.stderr)\n",
    "#\n",
    "# # 각 경로에 대한 루프를 실행\n",
    "# for i in range(1, 25):\n",
    "#     if i in [21, 23]:\n",
    "#         continue\n",
    "#     img_path = f\"D:/Dog/noseprints/data/images/a0{i}\"\n",
    "#     label_path = f\"D:/Dog/noseprints/data/a0{i}.json\"\n",
    "#     run_command(img_path, label_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 이미지를 train 데이터셋과 test 데이터셋으로 구분\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = np.array(glob.glob(\"D:/Dog/noseprints/data/images/*.jpg\")) # 이미지 데이터\n",
    "y= np.array(glob.glob(\"D:/Dog/noseprints/data/labels/*.txt\")) # 라벨 데이터\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y, random_state=42)\n",
    "# stratify 사용 시 이미지 갯수가 너무 적으면 아예 분리가 안됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 이미지 파일과 라벨 파일을 정렬하고, 파일 이름을 기준으로 데이터셋을 분할\n",
    "import numpy as np\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def extract_filename(file_path):\n",
    "    return os.path.splitext(os.path.basename(file_path))[0]\n",
    "\n",
    "image_files = np.array(sorted(glob.glob(\"D:/Dog/noseprints/data/images/*.jpg\")))\n",
    "label_files = np.array(sorted(glob.glob(\"D:/Dog/noseprints/data/labels/*.txt\")))\n",
    "\n",
    "image_filenames = np.array([extract_filename(f) for f in image_files])\n",
    "label_filenames = np.array([extract_filename(f) for f in label_files])\n",
    "\n",
    "assert np.array_equal(image_filenames, label_filenames), \"이미지와 라벨 파일이 일치하지 않음.\"\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(image_files, label_files, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train 데이터셋과 test 데이터셋을 각각 train, test 폴더에 복사\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "def copy_files_to_folder(file_list, destination_folder):\n",
    "    for file in file_list:\n",
    "        shutil.copy(file, destination_folder)\n",
    "\n",
    "# Train 데이터셋과 Test 데이터셋을 각각 train, test 폴더에 복사\n",
    "train_image_folder = 'D:/Dog/noseprints/data/train/images/'\n",
    "train_label_folder = 'D:/Dog/noseprints/data/train/labels/'\n",
    "test_image_folder = 'D:/Dog/noseprints/data/test/images/'\n",
    "test_label_folder = 'D:/Dog/noseprints/data/test/labels/'\n",
    "\n",
    "# 폴더가 없는 경우 생성\n",
    "os.makedirs(train_image_folder, exist_ok=True)\n",
    "os.makedirs(train_label_folder, exist_ok=True)\n",
    "os.makedirs(test_image_folder, exist_ok=True)\n",
    "os.makedirs(test_label_folder, exist_ok=True)\n",
    "\n",
    "# 이미지 파일 복사\n",
    "copy_files_to_folder(X_train, train_image_folder)\n",
    "copy_files_to_folder(X_test, test_image_folder)\n",
    "\n",
    "# 라벨 파일 복사\n",
    "copy_files_to_folder(y_train, train_label_folder)\n",
    "copy_files_to_folder(y_test, test_label_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 이미지 resize 시 라벨링 데이터도 같이 수정해야 함..진행하지 않기로 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# yaml 파일 경로 : D:/Dog/noseprints/dog.yaml\n",
    "# test data의 경우, 라벨링을 정보를 활용하지 않음\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# yaml 파일을 읽어서 데이터셋을 생성\n",
    "with open(\"D:/Dog/noseprints/dog.yaml\", 'r') as yaml_file:\n",
    "    data = yaml.load(yaml_file, Loader=yaml.FullLoader)\n",
    "    print(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# yolov5로 경로이동\n",
    "cd ../yolov5\n",
    "\n",
    "# train\n",
    "'''\n",
    "python train.py \\\n",
    "--img 이미지 사이즈 \\\n",
    "--batch 배치 사이즈 \\\n",
    "--data custom.yaml 파일경로 \\\n",
    "--cfg 사용할 모델의 yaml 파일경로 \\\n",
    "--weights 학습에 사용할 모델 \\\n",
    "--name 학습된 정보를 runs 폴더 안에 저장할 이름 \\\n",
    "--project wanbd에 저장할 프로젝트명\n",
    "'''\n",
    "\n",
    "# # 여러 파라미터 중 핵심만 사용\n",
    "# python train.py \\\n",
    "# --data ../starbucks/custom.yaml \\\n",
    "# --weights yolov5m.pt\n",
    "#\n",
    "# # 조금 더 조절하고 싶은 경우\n",
    "# python train.py \\\n",
    "# --data ../starbucks/custom.yaml \\\n",
    "# --weights yolov5m.pt \\\n",
    "# --batch ?? \\\n",
    "# --epochs ??? \\\n",
    "# --name starbucks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T08:56:36.680757800Z",
     "start_time": "2023-05-11T08:56:36.340471200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clear_session\n",
    "from tensorflow.keras import backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 학습된 모델을 이용하여 train\n",
    "!python train.py --batch 16 --epochs 50 --data D:/Dog/dog.yaml --weights yolov5m.pt --name dog1 --hyp hyp.yaml --cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hyp = {'lr0': 0.01,  # initial learning rate\n",
    "       'lrf': 0.2,  # final learning rate\n",
    "       'momentum': 0.937,  # SGD momentum\n",
    "       'weight_decay': 0.0005,  # optimizer weight decay\n",
    "       'warmup_epochs': 3.0,  # warmup epochs\n",
    "       'warmup_momentum': 0.8,  # warmup initial momentum\n",
    "       'warmup_bias_lr': 0.1,  # warmup initial bias lr\n",
    "       'box': 0.05,  # box loss gain\n",
    "       'cls': 0.5,  # cls loss gain\n",
    "       'cls_pw': 1.0,  # cls BCELoss positive_weight\n",
    "       'obj': 1.0,  # obj loss gain (scale with pixels)\n",
    "       'obj_pw': 1.0,  # obj BCELoss positive_weight\n",
    "       'iou_t': 0.20,  # IoU training threshold\n",
    "       'anchor_t': 4.0,  # anchor-multiple threshold\n",
    "       'fl_gamma': 0.0,  # focal loss gamma (efficientDet default gamma=1.5)\n",
    "       'hsv_h': 0.015,  # image HSV-Hue augmentation (fraction)\n",
    "       'hsv_s': 0.7,  # image HSV-Saturation augmentation (fraction)\n",
    "       'hsv_v': 0.4,  # image HSV-Value augmentation (fraction)\n",
    "       'degrees': 0.0,  # image rotation (+/- deg)\n",
    "       'translate': 0.1,  # image translation (+/- fraction)\n",
    "       'scale': 0.5,  # image scale (+/- gain)\n",
    "       'shear': 0.0,  # image shear (+/- deg)\n",
    "       'perspective': 0.0,  # image perspective (+/- fraction), range 0-0.001\n",
    "       'flipud': 0.0,  # image flip up-down (probability)\n",
    "       'fliplr': 0.5, # image flip left-right (probability)\n",
    "       'mosaic': 1.0, # image mixup (probability)\n",
    "       'mixup': 0.0 # image mixup (probability)\n",
    "       'copy_paste': 0.0, # segment copy-paste (probability)\n",
    "       'visual': 0.0, # visualize augmentation (probability)\n",
    "       'multi_scale': 0.0, # multi-scale training (probability)\n",
    "       'label_smoothing': 0.0, # label smoothing (probability)\n",
    "       'enable_mixup': True, # enable mixup training\n",
    "       'enable_mosaic': True, # enable mosaic training\n",
    "       'enable_cutmix': False, # enable cutmix training\n",
    "       'enable_copypaste': False, # enable cutmix training\n",
    "       'input_size': 640, # input size\n",
    "       'random_size': False, # use multi-size training\n",
    "       'random_size_min': 14, # min input size (pixels)\n",
    "       'random_size_max': 26, # max input size (pixels)\n",
    "       'random_size_interval': 4, # interval between sizes (pixels)\n",
    "       'random_size_mode': 'rect', # rect | ceil | floor\n",
    "       'letter_box': False, # letterbox resize\n",
    "       'mosaic_border': [-10, 10], # mosaic border (pixels)\n",
    "       'mixup_scale': (0.5, 1.5), # mixup scale\n",
    "       'mixup_ratio': 0.5, # mixup ratio\n",
    "       'cutmix_ratio': 0.5, # cutmix ratio\n",
    "       'cutmix_beta': 1.0, # cutmix beta\n",
    "       'cutmix_prob': 1.0, # cutmix probability\n",
    "       'cutmix_minmax': None, # cutmix min/max ratio\n",
    "       'cutmix_pad': False, # apply cutmix padding\n",
    "       'cutmix_pseudo': False, # use pseudo segmentation\n",
    "       'cutmix_criterion': 'iou', # cutmix IoU loss type (iou | iou_loss | bce)\n",
    "       'cutmix_threshold': 0.5, # cutmix IoU threshold\n",
    "       'cutmix_mosaic': False, # apply cutmix mosaic\n",
    "       'early_stop': True, # early stop training\n",
    "       'early_stop_patience': 10, # early stop patience epochs\n",
    "       'gradient_accumulation_steps': 1, # gradient accumulation steps\n",
    "       'workers': 8, # dataloader threads\n",
    "       'image_weights': False, # use weighted image selection for training\n",
    "       'compute_loss': None, # compute loss (train, val, test, None)\n",
    "       'save_optimizer': True, # save optimizer to file\n",
    "       'save_optimizer_path': '', # optimizer file path\n",
    "       'save_optimizer_type': 'torch', # optimizer file type (torch, state_dict)\n",
    "       'save_optimizer_every_epoch': False, # save optimizer every epoch\n",
    "       'save_optimizer_every_epoch_path': '', # optimizer every epoch file path\n",
    "       'save_optimizer_every_epoch_type': 'torch', # optimizer every epoch file type (torch, state_dict)\n",
    "       'save_optimizer_last': True, # save optimizer to last checkpoint\n",
    "       'save_optimizer_last_path': '', # optimizer last file path\n",
    "       'save_optimizer_last_type': 'torch', # optimizer last file type (torch, state_dict)\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lr0: 0.01  # initial learning rate (SGD=1E-2, Adam=1E-3)\n",
    "# lrf: 0.01  # final OneCycleLR learning rate (lr0 * lrf)\n",
    "# momentum: 0.937  # SGD momentum/Adam beta1\n",
    "# weight_decay: 0.0005  # optimizer weight decay 5e-4\n",
    "# warmup_epochs: 3.0  # warmup epochs (fractions ok)\n",
    "# warmup_momentum: 0.8  # warmup initial momentum\n",
    "# warmup_bias_lr: 0.1  # warmup initial bias lr\n",
    "# box: 0.05  # box loss gain\n",
    "# cls: 0.5  # cls loss gain\n",
    "# cls_pw: 1.0  # cls BCELoss positive_weight\n",
    "# obj: 1.0  # obj loss gain (scale with pixels)\n",
    "# obj_pw: 1.0  # obj BCELoss positive_weight\n",
    "# iou_t: 0.20  # IoU training threshold\n",
    "# anchor_t: 4.0  # anchor-multiple threshold\n",
    "# # anchors: 3  # anchors per output layer (0 to ignore)\n",
    "# fl_gamma: 0.0  # focal loss gamma (efficientDet default gamma=1.5)\n",
    "# hsv_h: 0.015  # image HSV-Hue augmentation (fraction)\n",
    "# hsv_s: 0.7  # image HSV-Saturation augmentation (fraction)\n",
    "# hsv_v: 0.4  # image HSV-Value augmentation (fraction)\n",
    "# degrees: 0.5  # image rotation (+/- deg)\n",
    "# translate: 0.1  # image translation (+/- fraction)\n",
    "# scale: 0.5  # image scale (+/- gain)\n",
    "# shear: 0.0  # image shear (+/- deg)\n",
    "# perspective: 0.0  # image perspective (+/- fraction), range 0-0.001\n",
    "# flipud: 0.5  # image flip up-down (probability)\n",
    "# fliplr: 0.5  # image flip left-right (probability)\n",
    "# mosaic: 1.0  # image mosaic (probability)\n",
    "# mixup: 0.0  # image mixup (probability)\n",
    "# copy_paste: 0.0  # segment copy-paste (probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T18:21:34.735196200Z",
     "start_time": "2023-05-08T18:21:34.580187300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train/box_loss</th>\n",
       "      <th>train/obj_loss</th>\n",
       "      <th>train/cls_loss</th>\n",
       "      <th>metrics/precision</th>\n",
       "      <th>metrics/recall</th>\n",
       "      <th>metrics/mAP_0.5</th>\n",
       "      <th>metrics/mAP_0.5:0.95</th>\n",
       "      <th>val/box_loss</th>\n",
       "      <th>val/obj_loss</th>\n",
       "      <th>val/cls_loss</th>\n",
       "      <th>x/lr0</th>\n",
       "      <th>x/lr1</th>\n",
       "      <th>x/lr2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.108170</td>\n",
       "      <td>0.030438</td>\n",
       "      <td>0.081701</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099950</td>\n",
       "      <td>0.023176</td>\n",
       "      <td>0.076658</td>\n",
       "      <td>0.088300</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.094074</td>\n",
       "      <td>0.029610</td>\n",
       "      <td>0.077287</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>0.19444</td>\n",
       "      <td>0.017247</td>\n",
       "      <td>0.004573</td>\n",
       "      <td>0.079230</td>\n",
       "      <td>0.022076</td>\n",
       "      <td>0.076168</td>\n",
       "      <td>0.075433</td>\n",
       "      <td>0.002433</td>\n",
       "      <td>0.002433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.081843</td>\n",
       "      <td>0.028148</td>\n",
       "      <td>0.073499</td>\n",
       "      <td>0.002666</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.012080</td>\n",
       "      <td>0.004304</td>\n",
       "      <td>0.071837</td>\n",
       "      <td>0.020123</td>\n",
       "      <td>0.077275</td>\n",
       "      <td>0.062288</td>\n",
       "      <td>0.003288</td>\n",
       "      <td>0.003288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.078729</td>\n",
       "      <td>0.026634</td>\n",
       "      <td>0.071605</td>\n",
       "      <td>0.006348</td>\n",
       "      <td>0.61111</td>\n",
       "      <td>0.032620</td>\n",
       "      <td>0.009898</td>\n",
       "      <td>0.064968</td>\n",
       "      <td>0.018708</td>\n",
       "      <td>0.077026</td>\n",
       "      <td>0.048867</td>\n",
       "      <td>0.003867</td>\n",
       "      <td>0.003867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.073708</td>\n",
       "      <td>0.026890</td>\n",
       "      <td>0.068633</td>\n",
       "      <td>0.020038</td>\n",
       "      <td>0.33333</td>\n",
       "      <td>0.043943</td>\n",
       "      <td>0.016537</td>\n",
       "      <td>0.064030</td>\n",
       "      <td>0.017530</td>\n",
       "      <td>0.076516</td>\n",
       "      <td>0.035168</td>\n",
       "      <td>0.004168</td>\n",
       "      <td>0.004168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.068477</td>\n",
       "      <td>0.024417</td>\n",
       "      <td>0.066950</td>\n",
       "      <td>0.006280</td>\n",
       "      <td>0.69444</td>\n",
       "      <td>0.022526</td>\n",
       "      <td>0.010168</td>\n",
       "      <td>0.058859</td>\n",
       "      <td>0.016464</td>\n",
       "      <td>0.074885</td>\n",
       "      <td>0.021192</td>\n",
       "      <td>0.004191</td>\n",
       "      <td>0.004191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.068019</td>\n",
       "      <td>0.024267</td>\n",
       "      <td>0.068923</td>\n",
       "      <td>0.008727</td>\n",
       "      <td>0.75000</td>\n",
       "      <td>0.037050</td>\n",
       "      <td>0.012387</td>\n",
       "      <td>0.063993</td>\n",
       "      <td>0.015791</td>\n",
       "      <td>0.074476</td>\n",
       "      <td>0.006938</td>\n",
       "      <td>0.003938</td>\n",
       "      <td>0.003938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.064715</td>\n",
       "      <td>0.022515</td>\n",
       "      <td>0.068203</td>\n",
       "      <td>0.011284</td>\n",
       "      <td>0.91667</td>\n",
       "      <td>0.127930</td>\n",
       "      <td>0.042809</td>\n",
       "      <td>0.061326</td>\n",
       "      <td>0.015446</td>\n",
       "      <td>0.073747</td>\n",
       "      <td>0.003070</td>\n",
       "      <td>0.003070</td>\n",
       "      <td>0.003070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.059321</td>\n",
       "      <td>0.023847</td>\n",
       "      <td>0.065046</td>\n",
       "      <td>0.009374</td>\n",
       "      <td>0.91667</td>\n",
       "      <td>0.105400</td>\n",
       "      <td>0.049029</td>\n",
       "      <td>0.050328</td>\n",
       "      <td>0.015684</td>\n",
       "      <td>0.071900</td>\n",
       "      <td>0.003070</td>\n",
       "      <td>0.003070</td>\n",
       "      <td>0.003070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.052544</td>\n",
       "      <td>0.024765</td>\n",
       "      <td>0.065485</td>\n",
       "      <td>0.008812</td>\n",
       "      <td>0.91667</td>\n",
       "      <td>0.188170</td>\n",
       "      <td>0.103060</td>\n",
       "      <td>0.045527</td>\n",
       "      <td>0.015538</td>\n",
       "      <td>0.071760</td>\n",
       "      <td>0.002080</td>\n",
       "      <td>0.002080</td>\n",
       "      <td>0.002080</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  epoch        train/box_loss        train/obj_loss  \\\n",
       "0                     0              0.108170              0.030438   \n",
       "1                     1              0.094074              0.029610   \n",
       "2                     2              0.081843              0.028148   \n",
       "3                     3              0.078729              0.026634   \n",
       "4                     4              0.073708              0.026890   \n",
       "5                     5              0.068477              0.024417   \n",
       "6                     6              0.068019              0.024267   \n",
       "7                     7              0.064715              0.022515   \n",
       "8                     8              0.059321              0.023847   \n",
       "9                     9              0.052544              0.024765   \n",
       "\n",
       "         train/cls_loss     metrics/precision        metrics/recall  \\\n",
       "0              0.081701              0.000000               0.00000   \n",
       "1              0.077287              0.001049               0.19444   \n",
       "2              0.073499              0.002666               0.50000   \n",
       "3              0.071605              0.006348               0.61111   \n",
       "4              0.068633              0.020038               0.33333   \n",
       "5              0.066950              0.006280               0.69444   \n",
       "6              0.068923              0.008727               0.75000   \n",
       "7              0.068203              0.011284               0.91667   \n",
       "8              0.065046              0.009374               0.91667   \n",
       "9              0.065485              0.008812               0.91667   \n",
       "\n",
       "        metrics/mAP_0.5  metrics/mAP_0.5:0.95          val/box_loss  \\\n",
       "0              0.000000              0.000000              0.099950   \n",
       "1              0.017247              0.004573              0.079230   \n",
       "2              0.012080              0.004304              0.071837   \n",
       "3              0.032620              0.009898              0.064968   \n",
       "4              0.043943              0.016537              0.064030   \n",
       "5              0.022526              0.010168              0.058859   \n",
       "6              0.037050              0.012387              0.063993   \n",
       "7              0.127930              0.042809              0.061326   \n",
       "8              0.105400              0.049029              0.050328   \n",
       "9              0.188170              0.103060              0.045527   \n",
       "\n",
       "           val/obj_loss          val/cls_loss                 x/lr0  \\\n",
       "0              0.023176              0.076658              0.088300   \n",
       "1              0.022076              0.076168              0.075433   \n",
       "2              0.020123              0.077275              0.062288   \n",
       "3              0.018708              0.077026              0.048867   \n",
       "4              0.017530              0.076516              0.035168   \n",
       "5              0.016464              0.074885              0.021192   \n",
       "6              0.015791              0.074476              0.006938   \n",
       "7              0.015446              0.073747              0.003070   \n",
       "8              0.015684              0.071900              0.003070   \n",
       "9              0.015538              0.071760              0.002080   \n",
       "\n",
       "                  x/lr1                 x/lr2  \n",
       "0              0.001300              0.001300  \n",
       "1              0.002433              0.002433  \n",
       "2              0.003288              0.003288  \n",
       "3              0.003867              0.003867  \n",
       "4              0.004168              0.004168  \n",
       "5              0.004191              0.004191  \n",
       "6              0.003938              0.003938  \n",
       "7              0.003070              0.003070  \n",
       "8              0.003070              0.003070  \n",
       "9              0.002080              0.002080  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train 결과 확인\n",
    "path = 'D:/Dog/yolov5/runs/train/dog12/results.csv'\n",
    "df = pd.read_csv(path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 학습된 모델을 이용해 test\n",
    "!python D:/Dog/yolov5/detect.py --weights D:/Dog/yolov5/runs/train/dog12/weights/best.pt --source D:/Dog/noseprints/data/test/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# exp3 전부 no detection...데이터가 턱없이 모자르다는 것을 확인"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
